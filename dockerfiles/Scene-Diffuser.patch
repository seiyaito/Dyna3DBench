diff --git a/configs/default.yaml b/configs/default.yaml
index 278bebe..55603d4 100644
--- a/configs/default.yaml
+++ b/configs/default.yaml
@@ -24,5 +24,5 @@ gpu: 0
 
 ## for saving model
 save_model_interval: 1
-save_model_seperately: false
-save_scene_model: false # save scene model or not, important!!!
+save_model_seperately: true
+save_scene_model: true # save scene model or not, important!!!
diff --git a/configs/model/unet.yaml b/configs/model/unet.yaml
index 2126f2a..447bb59 100644
--- a/configs/model/unet.yaml
+++ b/configs/model/unet.yaml
@@ -18,6 +18,6 @@ scene_model:
   use_color: ${task.dataset.use_color}
   use_normal: ${task.dataset.use_normal}
   num_points: ${task.dataset.num_points}
-  pretrained_weights: /home/wangzan/Outputs/point_transformer.scannet/outputs/2022-04-13_18-29-56_POINTTRANS_C_32768/model.pth
-  pretrained_weights_slurm: /home/wangzan/scratch/Outputs/point_transformer.scannet/outputs/2022-04-13_18-29-56_POINTTRANS_C_32768/model.pth
-freeze_scene_model: true
\ No newline at end of file
+  pretrained_weights: /data/checkpoints/2022-04-13_18-29-56_POINTTRANS_C_32768/model.pth
+  pretrained_weights_slurm: /data/checkpoints/2022-04-13_18-29-56_POINTTRANS_C_32768/model.pth
+freeze_scene_model: true
diff --git a/configs/task/motion_gen.yaml b/configs/task/motion_gen.yaml
index 45ab804..1c10f5e 100644
--- a/configs/task/motion_gen.yaml
+++ b/configs/task/motion_gen.yaml
@@ -15,7 +15,7 @@ test:
 ## dataset
 has_observation: false
 dataset:
-  name: LEMOMotion
+  name: LEMOMotionDummy
   desc: '[LEMO Motion] -- dataset used for motion generation conditioned on 3D scene'
   modeling_keys: ['transl', 'global_orient', 'betas', 'body_pose'] #, 'left_hand_pose', 'right_hand_pose']
   has_observation: ${task.has_observation}

@@ -26,14 +26,14 @@ dataset:
   use_color: true
   use_normal: false
   start_end_dist_threshold: 0.2
-  data_dir: /home/wangzan/Data/LEMO/PROX_temporal/PROX_temporal/PROXD_temp
-  data_dir_slurm: /home/wangzan/scratch/Data/LEMO/PROX_temporal/PROX_temporal/PROXD_temp
-  smpl_dir: /home/wangzan/Data/SHADE/models_smplx_v1_1/models/
-  smpl_dir_slurm: /home/wangzan/scratch/Data/SHADE/models_smplx_v1_1/models/
-  prox_dir: /home/wangzan/Data/SHADE/PROX/
-  prox_dir_slurm: /home/wangzan/scratch/Data/SHADE/PROX/
-  vposer_dir: '/home/wangzan/Data/SHADE/V02_05/'
-  vposer_dir_slurm: '/home/wangzan/scratch/Data/SHADE/V02_05/'
+  data_dir: /data/LEMO/PROX_temporal/PROXD_temp
+  data_dir_slurm: /data/LEMO/PROX_temporal/PROXD_temp
+  smpl_dir: /data/SMPLX/models/
+  smpl_dir_slurm: /data/SMPLX/models/
+  prox_dir: /data/PROX/
+  prox_dir_slurm: /data/PROX/
+  vposer_dir: /data/SMPLX/V02_05/
+  vposer_dir_slurm: /data/SMPLX/V02_05/
   smplx_pca_comps: 12
   smplx_model_device: cpu
 
@@ -53,7 +53,7 @@ visualizer:
   vis_case_num: 32
   ksample: 2 # sample k case in each case
   vis_denoising: false # visualize denoising process
-  save_mesh: false
+  save_mesh: true
   ## visualization config used in training
   visualize: false
   interval: 1
diff --git a/preprocessing/prox/prox_scene.py b/preprocessing/prox/prox_scene.py
index ca5b29d..3595af2 100644
--- a/preprocessing/prox/prox_scene.py
+++ b/preprocessing/prox/prox_scene.py
@@ -7,8 +7,8 @@ from easydict import EasyDict
 from plyfile import PlyData, PlyElement
 import trimesh
 
-scene_dir = '/home/wangzan/Data/SHADE/PROX/scenes/'
-preprocess_scenes_dir = '/home/wangzan/Data/SHADE/PROX/preprocess_scenes/'
+scene_dir = '/data/PROX/scenes/'
+preprocess_scenes_dir = '/data/PROX/preprocess_scenes/'
 scene_name = ['BasementSittingBooth', 'MPH11', 'MPH112', 'MPH8', 'N0Sofa', 'N3Library', \
     'MPH16', 'MPH1Library', 'N0SittingBooth', 'N3OpenArea', 'N3Office', 'Werkraum']
 
@@ -53,4 +53,4 @@ if __name__=='__main__':
         out_filename = scene_name+'.npy' # scene0000_00.npy
         collect_one_scene_data_label(scene_name, os.path.join(preprocess_scenes_dir, out_filename))
 
-    print("done!")
\ No newline at end of file
+    print("done!")
diff --git a/datasets/__init__.py b/datasets/__init__.py
index 11f4e51..cadfc6a 100644
--- a/datasets/__init__.py
+++ b/datasets/__init__.py
@@ -1,4 +1,5 @@
 from .lemo_pose import LEMOPose
 from .multidex_shadowhand_ur import MultiDexShadowHandUR
 from .lemo_motion import LEMOMotion
+from .lemo_motion_dummy import LEMOMotionDummy
 from .scannet_path import ScanNetPath
diff --git a/datasets/lemo_motion.py b/datasets/lemo_motion.py
index 29e5c90..5360007 100644
--- a/datasets/lemo_motion.py
+++ b/datasets/lemo_motion.py
@@ -151,7 +151,9 @@ class LEMOMotion(Dataset):
             if not os.path.isdir(record_dir):
                 continue
             
-            scene_id, subject_id, _ = record_id.split('_')
+            token = record_id.split('_')
+            scene_id = "_".join(token[:-2])
+            subject_id = token[-2]
             if scene_id not in self.split:
                 continue

diff --git a/sample.py b/sample.py
index e70bbac..fbed67e 100644
--- a/sample.py
+++ b/sample.py
@@ -60,7 +60,7 @@ def main(cfg: DictConfig) -> None:
     ## prepare dataset for visual evaluation
     ## only load scene
     datasets = {
-        'test': create_dataset(cfg.task.dataset, 'test', cfg.slurm, case_only=True),
+        'test': create_dataset(cfg.task.dataset, 'test', cfg.slurm, case_only=True, **cfg.task.dataset.get("extra_args", {})),
     }
     for subset, dataset in datasets.items():
         logger.info(f'Load {subset} dataset size: {len(dataset)}')
@@ -101,4 +101,4 @@ if __name__ == '__main__':
     torch.cuda.manual_seed(seed)
     torch.cuda.manual_seed_all(seed)
 
-    main()
\ No newline at end of file
+    main()
diff --git a/datasets/lemo_motion_dummy.py b/datasets/lemo_motion_dummy.py
new file mode 100644
index 0000000..c8d2ae6
--- /dev/null
+++ b/datasets/lemo_motion_dummy.py
@@ -0,0 +1,268 @@
+from typing import Any, Tuple, Dict
+import os
+import json
+import pickle
+import trimesh
+import torch
+import numpy as np
+from torch.utils.data import Dataset, DataLoader
+from omegaconf import DictConfig
+
+from utils.smplx_utils import convert_smplx_verts_transfomation_matrix_to_body
+from utils.smplx_utils import SMPLXWrapper
+from datasets.transforms import make_default_transform
+from datasets.normalize import NormalizerPoseMotion
+from datasets.base import DATASET
+
+@DATASET.register()
+class LEMOMotionDummy(Dataset):
+    """ Dataset for motion generation, training with LEMO dataset
+    """
+
+    _train_split = ['BasementSittingBooth', 'MPH11', 'MPH112', 'MPH8', 'N0Sofa', 'N3Library', 'N3Office', 'Werkraum']
+    # _test_split = ['MPH16', 'MPH1Library', 'N0SittingBooth', 'N3OpenArea']
+    _all_split = ['BasementSittingBooth', 'MPH11', 'MPH112', 'MPH8', 'N0Sofa', 'N3Library', 'N3Office', 'Werkraum', 
+    'MPH16', 'MPH1Library', 'N0SittingBooth', 'N3OpenArea']
+    # _train_split = ['BasementSittingBooth']
+    _test_split = ['MPH16']
+
+    _female_subjects_ids = [162, 3452, 159, 3403]
+
+    def __init__(self, cfg: DictConfig, phase: str, slurm: bool, case_only: bool=False, **kwargs: Dict) -> None:
+        super(LEMOMotionDummy, self).__init__()
+        self.phase = phase
+        self.slurm = slurm
+        if 'specific_scene' in kwargs:
+            self.split = [kwargs['specific_scene']]
+        else:
+            if self.phase == 'train':
+                self.split = self._train_split
+            elif self.phase == 'test':
+                self.split = self._test_split
+            elif self.phase == 'all':
+                self.split = self._all_split
+            else:
+                raise Exception('Unsupported phase.')
+        self.horizon = cfg.horizon
+        self.frame_interval = cfg.frame_interval_train if self.phase == 'train' else cfg.frame_interval_test # interval sampling
+        self.modeling_keys = cfg.modeling_keys
+        self.num_points = cfg.num_points
+        self.use_color = cfg.use_color
+        self.use_normal = cfg.use_normal
+        self.start_end_dist_threshold = cfg.start_end_dist_threshold
+        self.transform = make_default_transform(cfg, phase)
+        self.has_observation = cfg.has_observation
+
+        ## resource folders
+        self.data_dir = cfg.data_dir_slurm if self.slurm else cfg.data_dir
+        self.smpl_dir = cfg.smpl_dir_slurm if self.slurm else cfg.smpl_dir
+        self.prox_dir = cfg.prox_dir_slurm if self.slurm else cfg.prox_dir
+        self.prox_scene_ply = os.path.join(self.prox_dir, 'scenes')
+        self.prox_scene_npy = os.path.join(self.prox_dir, 'preprocess_scenes')
+        self.prox_scene_sdf = os.path.join(self.prox_dir, 'sdf')
+        self.prox_cam2world = os.path.join(self.prox_dir, 'cam2world')
+
+        self.SMPLX = SMPLXWrapper(self.smpl_dir, cfg.smplx_model_device, cfg.smplx_pca_comps) # singleton
+
+        self.normalizer = None
+        self.repr_type = cfg.repr_type
+        if cfg.use_normalize:
+            cur_dir = os.path.dirname(os.path.abspath(__file__))
+            if self.repr_type == 'absolute':
+                with open(os.path.join(cur_dir, 'lemo/normalization.pkl'), 'rb') as fp:
+                    data = pickle.load(fp)
+                xmin = data['xmin'].astype(np.float32)
+                xmax = data['xmax'].astype(np.float32)
+                self.normalizer = NormalizerPoseMotion((xmin, xmax))
+            elif self.repr_type == 'relative':
+                with open(os.path.join(cur_dir, 'lemo/normalization_relative_v2.pkl'), 'rb') as fp:
+                    data = pickle.load(fp)
+                xmin = data['xmin'].astype(np.float32)
+                xmax = data['xmax'].astype(np.float32)
+
+                ## in relative repr and not has observation setting, we need to model the first frame
+                ## which is represented in absolute representation.
+                if not self.has_observation:
+                    with open(os.path.join(cur_dir, 'lemo/normalization.pkl'), 'rb') as fp:
+                        data = pickle.load(fp)
+                    abs_xmin = data['xmin']
+                    abs_xmax = data['xmax']
+
+                    xmin = np.vstack((
+                        abs_xmin.reshape(1, -1),
+                        np.repeat(xmin.reshape(1, -1), self.horizon + 1, axis=0)
+                    ))
+                    xmax = np.vstack((
+                        abs_xmax.reshape(1, -1),
+                        np.repeat(xmax.reshape(1, -1), self.horizon + 1, axis=0)
+                    ))
+                self.normalizer = NormalizerPoseMotion((xmin, xmax))
+            else:
+                raise Exception('Unsupported repr type.')
+        ## load data
+        self._pre_load_data(case_only)
+
+    def _pre_load_data(self, case_only: bool) -> None:
+        """ Load dataset
+
+        Args:
+            case_only: only load single case for testing, if ture, the dataset will be smaller.
+                        This is useful in after-training visual evaluation.
+        """
+        self.scene_meshes = {}
+        self.scene_pcds = {}
+        self.scene_sdf = {}
+        self.cam_trans = {}
+        self.motions = []
+        
+        ## load original mesh
+        ## load preprocessed scene point cloud
+        ## load camera transformation
+        for s in self.split:
+            scene_mesh = trimesh.load(os.path.join(self.prox_scene_ply, s + '.ply'))
+            self.scene_meshes[s] = scene_mesh
+            
+            scene_pcd = np.load(os.path.join(self.prox_scene_npy, s + '.npy'))
+            self.scene_pcds[s] = scene_pcd.astype(np.float32)
+
+            with open(os.path.join(self.prox_scene_sdf, s + '.json')) as f:
+                sdf_data = json.load(f)
+                grid_min = np.array(sdf_data['min'], dtype=np.float32)
+                grid_max = np.array(sdf_data['max'], dtype=np.float32)
+                grid_dim = sdf_data['dim']
+            grid_sdf = np.load(os.path.join(self.prox_scene_sdf, s + '_sdf.npy')).reshape(grid_dim, grid_dim, grid_dim)
+            self.scene_sdf[s] = {'grid_min': grid_min, 'grid_max': grid_max, 'grid_dim': grid_dim, 'grid_sdf': grid_sdf}
+            
+            with open(os.path.join(self.prox_cam2world, s + '.json'), 'r') as f:
+                trans = np.array(json.load(f))
+            self.cam_trans[s] = trans.astype(np.float32)
+
+            ## load motions of all available sequences
+            subject_gender = 'male'
+
+            motion_info = {
+                'record': "dummy", 'scene': s, 'gender': subject_gender,
+                'betas': [],
+                'global_orient': [],
+                'transl': [],
+                'left_hand_pose': [],
+                'right_hand_pose': [],
+                'body_pose': [],
+                'cur_transl': [],
+                'cur_global_orient': [],
+                'pelvis': [],
+            }
+
+            params = {
+                'camera_rotation': np.eye(3, dtype=np.float32),
+                'camera_translation': np.zeros((1, 3), dtype=np.float32),
+                'betas': np.zeros((1, 10), dtype=np.float32),
+                'global_orient': np.ones((1, 3), dtype=np.float32),
+                'transl': np.zeros((1, 3), dtype=np.float32),
+                'left_hand_pose': np.zeros((1, 12), dtype=np.float32),
+                'right_hand_pose': np.zeros((1, 12), dtype=np.float32),
+                'jaw_pose': np.zeros((1, 3), dtype=np.float32),
+                'leye_pose': np.zeros((1, 3), dtype=np.float32),
+                'reye_pose': np.zeros((1, 3), dtype=np.float32),
+                'expression': np.zeros((1, 10), dtype=np.float32),
+                'pose_embedding': np.zeros((1, 32), dtype=np.float32),
+                'body_pose': np.zeros((1, 63), dtype=np.float32),
+            }
+
+            torch_param = {}
+            for key in param:
+                if key not in ['pose_embedding', 'camera_rotation', 'camera_translation']:
+                    torch_param[key] = torch.tensor(param[key]) # <1, FDim>
+                    if key in motion_info:
+                        motion_info[key].append(param[key].squeeze(axis=0)) # <FDim>
+
+            ## We fix the scene and transform the smplx body with the camera transformation matrix,
+            ## which is different from the PROX official code tranforming scenes.
+            ## So we first need to compute the body pelvis location, see more demonstration at
+            ## https://www.dropbox.com/scl/fi/zkatuv5shs8d4tlwr8ecc/Change-parameters-to-new-coordinate-system.paper?dl=0&rlkey=lotq1sh6wzkmyttisc05h0in0
+            _, _, joints = self.SMPLX.run(torch_param, subject_gender)
+            pelvis = joints[:, 0, :].numpy()
+
+            cur_transl, cur_global_orient = convert_smplx_verts_transfomation_matrix_to_body(
+                self.cam_trans[s],
+                param['transl'].squeeze(axis=0),
+                param['global_orient'].squeeze(axis=0),
+                pelvis.squeeze(axis=0)
+            )
+
+            motion_info['cur_transl'].append(cur_transl.astype(np.float32))
+            motion_info['cur_global_orient'].append(cur_global_orient.astype(np.float32))
+            motion_info['pelvis'].append(pelvis.astype(np.float32).squeeze(axis=0))
+
+            ## convert list to numpy array
+            for key in motion_info:
+                if isinstance(motion_info[key], list):
+                    motion_info[key] = np.array(motion_info[key]).reshape((1, -1))
+
+            self.motions.append(motion_info)
+
+        ## segment motion to fixed horizon
+        self.indices = []
+        for i in range(len(self.split)):
+            self.indices.append((i, 0, self.horizon + 2))
+
+    def __len__(self):
+        return len(self.indices)
+    
+    def __getitem__(self, index: Any) -> Tuple:
+        scene_idx, start, end = self.indices[index]
+        scene_id = self.split[scene_idx]
+
+        ## load data, containing scene point cloud and point pose
+        scene_pc = self.scene_pcds[scene_id]
+        scene_sdf_data = self.scene_sdf[scene_id]
+        scene_grid_min = scene_sdf_data['grid_min']
+        scene_grid_max = scene_sdf_data['grid_max']
+        scene_grid_dim = scene_sdf_data['grid_dim']
+        scene_grid_sdf = scene_sdf_data['grid_sdf']
+        cam_tran = self.cam_trans[scene_id]
+
+        ## randomly resample points
+        assert self.phase != 'train'
+        np.random.seed(0)  # resample point cloud with a fixed random seed
+        idx = np.random.permutation(len(scene_pc))
+        scene_pc = scene_pc[idx[:self.num_points]]
+
+        ## format point cloud xyz and feature
+        xyz = scene_pc[:, 0:3]
+        feat = scene_pc[:, 3:3]
+
+        if self.use_color:
+            color = scene_pc[:, 3:6] / 255.
+            feat = np.concatenate([feat, color], axis=-1)
+
+        if self.use_normal:
+            normal = scene_pc[:, 6:9]
+            feat = np.concatenate([feat, normal], axis=-1)
+        
+        ## format smplx parameters
+        smplx_params = (
+            self.motions[scene_idx]['cur_transl'].repeat(end - start + 1, axis=0),
+            self.motions[scene_idx]['cur_global_orient'].repeat(end - start + 1, axis=0),
+            self.motions[scene_idx]['betas'].repeat(end - start + 1, axis=0),
+            self.motions[scene_idx]['body_pose'].repeat(end - start + 1, axis=0),
+            self.motions[scene_idx]['left_hand_pose'].repeat(end - start + 1, axis=0),
+            self.motions[scene_idx]['right_hand_pose'].repeat(end - start + 1, axis=0),
+        )
+        
+        data = {
+            'x': smplx_params, 
+            'pos': xyz, 
+            'feat': feat, 
+            'cam_tran': cam_tran, 
+            'scene_id': scene_id, 
+            'gender': self.motions[scene_idx]['gender'],
+            'origin_cam_tran': cam_tran, 
+            'origin_pelvis': self.motions[scene_idx]['pelvis'].repeat(end - start + 1, axis=0),
+            'origin_transl': self.motions[scene_idx]['transl'].repeat(end - start + 1, axis=0),
+            'origin_global_orient': self.motions[scene_idx]['global_orient'].repeat(end - start + 1, axis=0),
+            's_grid_sdf': scene_grid_sdf,
+            's_grid_min': scene_grid_min,
+            's_grid_max': scene_grid_max,
+            's_grid_dim': scene_grid_dim,
+        }
+
+        if self.transform is not None:
+            data = self.transform(data, modeling_keys=self.modeling_keys, repr_type=self.repr_type, normalizer=self.normalizer, motion=True)
+            
+        return data
+
+    def get_dataloader(self, **kwargs):
+        return DataLoader(self, **kwargs)
